# -*- coding: utf-8 -*-
"""Binoculars_Semeval - copia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-2pEjNzP51IkRBzM4AcBlEJUgkNhQT-S
"""

# Instalar una versión específica precompilada de tokenizers
!pip install tokenizers==0.15.0

# Instalar transformers compatible
!pip install transformers>=4.35.0

# Finalmente instalar Binoculars sin dependencias
!pip install --no-deps git+https://github.com/ahans30/Binoculars.git

# Instalar otras dependencias manualmente si faltan
#!pip install torch accelerate

import torch

# Verificar si hay GPU disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Usando: {device}")

if device == "cuda":
    print(f"✓ GPU: {torch.cuda.get_device_name(0)}")
    print(f"✓ VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

#sí funciona

from binoculars import Binoculars

# Inicializar Binoculars
bino = Binoculars() # así demora mucho porque descarga los modelos Falcon-7B que son muy grandes

print("✓ Modelos cargados en GPU") if device == "cuda" else print("✓ Modelos cargados en CPU")

"""from binoculars import Binoculars

# Binoculars ya importa internamente todo lo que necesita de transformers y torch.

# aquí usa modelos más ligeros para español
bino = Binoculars(
    observer_name_or_path="xlm-roberta-base",
    performer_name_or_path="xlm-roberta-large"
)

# PARA ESPAÑOL

---

from binoculars import Binoculars

# Usar modelos MÁS PEQUEÑOS si estás en CPU
if device == "cpu":
    print("⚠️ Usando CPU - será lento. Usando modelos pequeños...")
    bino = Binoculars(
        observer_name_or_path="distilgpt2",  # Muy pequeño ~350MB
        performer_name_or_path="gpt2"        # Pequeño ~500MB
    )
else:
    print("✓ GPU detectada - usando modelos más grandes")
    bino = Binoculars(
        observer_name_or_path="xlm-roberta-base",
        performer_name_or_path="xlm-roberta-large"
    )


# Después de inicializar, verificar si estpa usando GPU:
print(f"Observer device: {bino.observer_model.device}")
print(f"Performer device: {bino.performer_model.device}")

# Probar
texto_prueba = "La inteligencia artificial está transformando la educación superior."
score = bino.compute_score(texto_prueba)
print(f"Binoculars Score: {score:.4f}")
print(f"Predicción: {'IA' if score < 0.5 else 'Humano'}")

# PARA INGLÉS

---

from binoculars import Binoculars

# Inicializar Binoculars
bino = Binoculars(
    observer_name_or_path="gpt2",
    performer_name_or_path="gpt2-medium"
)

print("✓ Modelos cargados en GPU") if device == "cuda" else print("✓ Modelos cargados en CPU")
"""

# Probar
text = "The rapid advancement of artificial intelligence has revolutionized multiple industries."
#text = "In today's rapidly evolving digital landscape, artificial intelligence has emerged as a transformative force that is fundamentally reshaping the educational sector. It is important to note that educational institutions worldwide are increasingly leveraging AI-powered tools to enhance learning outcomes and streamline administrative processes. Moreover, these technological advancements have the potential to revolutionize traditional pedagogical approaches by providing personalized learning experiences tailored to individual student needs. Furthermore, the integration of AI in education facilitates data-driven decision-making, enabling educators to identify learning gaps and implement targeted interventions more effectively. However, it is crucial to acknowledge that while AI offers numerous benefits, there are also significant challenges that need to be addressed, including concerns about data privacy, algorithmic bias, and the potential displacement of human educators. Nevertheless, with proper governance frameworks and ethical guidelines, AI can serve as a powerful complement to human instruction rather than a replacement."
score = bino.compute_score(text)
print(f"\nScore: {score:.4f}")
print(f"Predicción: {bino.predict(text)}")

path_ft = "/content/drive/MyDrive/Research2025/Semeval2024/dfs_Preproc_Caract/"

import pandas as pd

df_test = pd.read_json(path_ft + "df_test_en_clean.jsonl", orient='records', lines=True)

df_test

#df_test = df_test.head(100)
#print(df_test.head())

import pandas as pd
from tqdm.notebook import tqdm

tqdm.pandas() # Enable .progress_apply() for pandas

import pandas as pd
from tqdm.notebook import tqdm

tqdm.pandas() # Enable .progress_apply() for pandas

def get_bino_prediction(text):
    """
    Computes the Binoculars score and uses bino.predict() for prediction.
    Returns a tuple (score, prediction_label).
    Handles potential errors during computation.
    """
    if not isinstance(text, str):
        return None, None # Handle non-string inputs
    try:
        score = bino.compute_score(text) # Still compute score for score_bino column
        bino_raw_prediction = bino.predict(text)

        # Convert bino.predict() string output to 1 (AI) or 0 (Human)
        #if "AI-Generated" in bino_raw_prediction:
        #    prediction_label = 1
        #else: # Assumes "human-generated" if not "AI-Generated"
        #    prediction_label = 0

        #return score, prediction_label
        return score, bino_raw_prediction

    except Exception as e:
        print(f"Error processing text (first 50 chars): {text[:50]}... Error: {e}")
        return None, None # Return None for both if an error occurs

print("Calculating Binoculars scores and predictions using bino.predict()...")

# Apply the new function to the 'text' column and store results
scores_predictions_series = df_test['text'].progress_apply(get_bino_prediction)

# Unpack the results into new columns, handling potential None values from errors
df_test['score_bino'] = scores_predictions_series.apply(lambda x: x[0] if x is not None else None)
df_test['prediction_bino'] = scores_predictions_series.apply(lambda x: x[1] if x is not None else None)

print("\nPrimeras 5 filas de df_test con las nuevas columnas (using bino.predict()):")
print(df_test[['text', 'score_bino', 'prediction_bino']].head())

print("\nDistribución de 'prediction_bino' (using bino.predict()):")
print(df_test['prediction_bino'].value_counts(dropna=False))

df_test

# Define the mapping for encoding the 'prediction_bino' column
mapping = {
    'Most likely human-generated': 0,
    'Most likely AI-generated': 1
}

# Apply the mapping to the 'prediction_bino' column
df_test['prediction_bino'] = df_test['prediction_bino'].map(mapping)

print("Primeras 5 filas de df_test con la columna 'prediction_bino' codificada:")
print(df_test[['prediction_bino']].head())

print("\nDistribución de la columna 'prediction_bino' codificada:")
print(df_test['prediction_bino'].value_counts(dropna=False))

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Filter out rows where prediction_bino is None, as these could not be processed
df_filtered = df_test.dropna(subset=['prediction_bino'])

y_true = df_filtered['label']
y_pred = df_filtered['prediction_bino'].astype(int) # Ensure predictions are integers

print("\n--- Binoculars Model Evaluation ---\n")

# Calculate and print the Classification Report
print("Classification Report:")
print(classification_report(y_true, y_pred))

# Calculate the Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

# Plot the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Human (0)', 'AI (1)'],
            yticklabels=['Human (0)', 'AI (1)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Binoculars Predictions')
plt.show()



